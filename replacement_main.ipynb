{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33104287",
   "metadata": {},
   "source": [
    "Alan MH Beem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4d7b3e",
   "metadata": {},
   "source": [
    "**Reflection on Pipeline**\n",
    "\n",
    "The challenge of getting this to run was rewarding, though the issue is not really 100% fixed, as the imports took longer than I was willing to wait.\n",
    "\n",
    "The world may not need another look at these penguins, though I will follow up with further pruning to the DenseNet model to minimize 100% accuracy, and / but, this is not the purpose of the tutorial. The purpose of the tutorial, and re. the target audience, seem to be displaying the capabilities of Tensorflow Extended (tfx) to an ML engineer or production-minded practitioner where the focus is how one would structure a production ML workflow using tfx.\n",
    "\n",
    "For example, a dedicated metadata filesystem is unnecessary for a single dataset, single tuning, \"pipeline\". The metadata involved is relatively static and limited in scope, and, in this notebook, a lightweight configuration file or explicit code-based specification may be sufficient. However, where repeated executions, provenance tracking, auditability, and artifact lineage become important, a structured metadata store provides clear advantages for reproducibility, caching, and orchestration. Thus, while the additional infrastructure may seem excessive for small-scale or educational use, it becomes more justified as system complexity and operational requirements increase.\n",
    "\n",
    "For production, it may be that many users want to access predictions from the set of current best models that the group has, or particular models, or, to route models through the production environment as is optimal w.r.t. cost. The revised pipeline below does all of that and is built to imitate the functionality of tfx. After completing this assignment using AI to generate most of the code, which I do find to be compatible with following along a tutorial-- in a sense, I'm thinking that a simple database would go the distance that I'd need for keeping track of many variations of models in a project, and comparing runs- it actually would have been quite convenient in my 410 projects, which had a network of Colab notebooks, but that might require a proper database solution. Another option would be a folder of folders each containing a simple JSON file describing the model hyperparameters and data used.\n",
    "\n",
    "\n",
    "**Specific Deliverables**\n",
    "\n",
    "1) Complete the tutorial\n",
    "\n",
    "2) Description of pipeline in tutorial:\n",
    "This pipeline ingests data, uses a user-provided function to train a model, pushes the model to a filesystem location == database destination (e.g. the database in the example below), runs the pipeline. The data is ingested in a normalized state, after preprocessing.\n",
    "\n",
    "3) Tutorial pipeline improvements:\n",
    "Data validation, feature engineering (including preprocessing into [0, 1] as is provided at the start in the tutorial, with unknown data leakage), and adding model analysis, such as visualization of model training, and comparisons of model performance. I'd like the parameters of the models to be more flexible (currently, no user-provided paramters make it to the models), but as-is, and as tfx seems to be, one could serve a point-and-click ML solution using tfx.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23387262",
   "metadata": {},
   "source": [
    "The below generated using ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a45b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# TFX-lite Notebook Pipeline Infrastructure (single cell)\n",
    "# - One SQLite DB, multiple tables:\n",
    "#   feature_sets, pipeline_specs, job_queue, runs, artifacts, metrics\n",
    "# - User provides config in a later cell by calling:\n",
    "#   register_feature_set(...), register_pipeline_spec(...), enqueue_job(...)\n",
    "# - Execution:\n",
    "#   process_next_job()  (or execute_pipeline(\"spec_name\") directly)\n",
    "# - Caching:\n",
    "#   Reuses prior run if (spec + hyperparams + data_fingerprint) matches\n",
    "# - Artifacts:\n",
    "#   artifacts/run_<run_id>/ (model + metrics + training_summary.png)\n",
    "# =========================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json, os, sqlite3, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Database                                                                                              ###\n",
    "class Store:\n",
    "    def __init__(self, db_path: str = \"metadata.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.conn = sqlite3.connect(db_path)\n",
    "        self.conn.execute(\"PRAGMA foreign_keys=ON;\")\n",
    "        self._init_schema()\n",
    "\n",
    "    def _init_schema(self) -> None:\n",
    "        self.conn.executescript(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS feature_sets(\n",
    "            name TEXT PRIMARY KEY,\n",
    "            label TEXT NOT NULL,\n",
    "            numeric_json TEXT NOT NULL,\n",
    "            categorical_json TEXT NOT NULL,\n",
    "            created_at TEXT NOT NULL\n",
    "        );\n",
    "\n",
    "        CREATE TABLE IF NOT EXISTS pipeline_specs(\n",
    "            name TEXT PRIMARY KEY,\n",
    "            csv TEXT NOT NULL,\n",
    "            feature_set TEXT NOT NULL,\n",
    "            model_type TEXT NOT NULL,\n",
    "            hyperparams_json TEXT NOT NULL,\n",
    "            created_at TEXT NOT NULL,\n",
    "            FOREIGN KEY(feature_set) REFERENCES feature_sets(name)\n",
    "        );\n",
    "\n",
    "        CREATE TABLE IF NOT EXISTS job_queue(\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            pipeline_name TEXT NOT NULL,\n",
    "            status TEXT NOT NULL, -- queued/running/done/failed\n",
    "            resources_json TEXT NOT NULL,\n",
    "            created_at TEXT NOT NULL,\n",
    "            started_at TEXT,\n",
    "            finished_at TEXT,\n",
    "            error TEXT,\n",
    "            FOREIGN KEY(pipeline_name) REFERENCES pipeline_specs(name)\n",
    "        );\n",
    "\n",
    "        CREATE TABLE IF NOT EXISTS runs(\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            pipeline_name TEXT NOT NULL,\n",
    "            timestamp TEXT NOT NULL,\n",
    "            params_json TEXT NOT NULL,\n",
    "            data_fingerprint TEXT NOT NULL,\n",
    "            cache_key TEXT NOT NULL UNIQUE,\n",
    "            status TEXT NOT NULL, -- done/failed\n",
    "            FOREIGN KEY(pipeline_name) REFERENCES pipeline_specs(name)\n",
    "        );\n",
    "\n",
    "        CREATE TABLE IF NOT EXISTS artifacts(\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            run_id INTEGER NOT NULL,\n",
    "            name TEXT NOT NULL,\n",
    "            path TEXT NOT NULL,\n",
    "            type TEXT NOT NULL,\n",
    "            FOREIGN KEY(run_id) REFERENCES runs(id)\n",
    "        );\n",
    "\n",
    "        CREATE TABLE IF NOT EXISTS metrics(\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            run_id INTEGER NOT NULL,\n",
    "            key TEXT NOT NULL,\n",
    "            value REAL NOT NULL,\n",
    "            FOREIGN KEY(run_id) REFERENCES runs(id)\n",
    "        );\n",
    "        \"\"\")\n",
    "        self.conn.commit()\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    @staticmethod\n",
    "    def _now() -> str:\n",
    "        return datetime.utcnow().isoformat()\n",
    "\n",
    "    @staticmethod\n",
    "    def _json(obj: Any) -> str:\n",
    "        return json.dumps(obj, sort_keys=True, separators=(\",\", \":\"))\n",
    "\n",
    "    @staticmethod\n",
    "    def _sha256_bytes(b: bytes) -> str:\n",
    "        return hashlib.sha256(b).hexdigest()\n",
    "\n",
    "    # ---------- register config ----------\n",
    "    def register_feature_set(self, name: str, label: str, numeric: List[str], categorical: List[str]) -> None:\n",
    "        self.conn.execute(\n",
    "            \"INSERT OR REPLACE INTO feature_sets(name,label,numeric_json,categorical_json,created_at) VALUES (?,?,?,?,?)\",\n",
    "            (name, label, self._json(numeric), self._json(categorical), self._now())\n",
    "        )\n",
    "        self.conn.commit()\n",
    "\n",
    "    def register_pipeline_spec(self, name: str, csv: str, feature_set: str, model_type: str, hyperparams: Dict[str, Any]) -> None:\n",
    "        self.conn.execute(\n",
    "            \"INSERT OR REPLACE INTO pipeline_specs(name,csv,feature_set,model_type,hyperparams_json,created_at) VALUES (?,?,?,?,?,?)\",\n",
    "            (name, csv, feature_set, model_type, self._json(hyperparams), self._now())\n",
    "        )\n",
    "        self.conn.commit()\n",
    "\n",
    "    # ---------- read config ----------\n",
    "    def get_feature_set(self, name: str) -> Dict[str, Any]:\n",
    "        row = self.conn.execute(\n",
    "            \"SELECT label,numeric_json,categorical_json FROM feature_sets WHERE name=?\",\n",
    "            (name,)\n",
    "        ).fetchone()\n",
    "        if not row:\n",
    "            raise KeyError(f\"feature_set not found: {name}\")\n",
    "        label, numeric_json, categorical_json = row\n",
    "        return {\"label\": label, \"numeric\": json.loads(numeric_json), \"categorical\": json.loads(categorical_json)}\n",
    "\n",
    "    def get_pipeline_spec(self, name: str) -> Dict[str, Any]:\n",
    "        row = self.conn.execute(\n",
    "            \"SELECT csv,feature_set,model_type,hyperparams_json FROM pipeline_specs WHERE name=?\",\n",
    "            (name,)\n",
    "        ).fetchone()\n",
    "        if not row:\n",
    "            raise KeyError(f\"pipeline_spec not found: {name}\")\n",
    "        csv, feature_set, model_type, hyperparams_json = row\n",
    "        return {\"name\": name, \"csv\": csv, \"feature_set\": feature_set, \"model_type\": model_type, \"hyperparams\": json.loads(hyperparams_json)}\n",
    "\n",
    "    # ---------- queue ----------\n",
    "    def enqueue_job(self, pipeline_name: str, resources: Optional[Dict[str, Any]] = None) -> int:\n",
    "        resources = resources or {\"runner\": \"local\", \"device\": \"cpu\"}\n",
    "        cur = self.conn.execute(\n",
    "            \"INSERT INTO job_queue(pipeline_name,status,resources_json,created_at) VALUES (?,?,?,?)\",\n",
    "            (pipeline_name, \"queued\", self._json(resources), self._now())\n",
    "        )\n",
    "        self.conn.commit()\n",
    "        return int(cur.lastrowid)\n",
    "\n",
    "    def pop_next_job(self) -> Optional[Dict[str, Any]]:\n",
    "        row = self.conn.execute(\n",
    "            \"SELECT id,pipeline_name,resources_json FROM job_queue WHERE status='queued' ORDER BY id ASC LIMIT 1\"\n",
    "        ).fetchone()\n",
    "        if not row:\n",
    "            return None\n",
    "        job_id, pipeline_name, resources_json = row\n",
    "        self.conn.execute(\n",
    "            \"UPDATE job_queue SET status='running', started_at=? WHERE id=?\",\n",
    "            (self._now(), job_id)\n",
    "        )\n",
    "        self.conn.commit()\n",
    "        return {\"id\": int(job_id), \"pipeline_name\": pipeline_name, \"resources\": json.loads(resources_json)}\n",
    "\n",
    "    def finish_job(self, job_id: int, status: str, error: Optional[str] = None) -> None:\n",
    "        self.conn.execute(\n",
    "            \"UPDATE job_queue SET status=?, finished_at=?, error=? WHERE id=?\",\n",
    "            (status, self._now(), error, job_id)\n",
    "        )\n",
    "        self.conn.commit()\n",
    "\n",
    "    # ---------- runs / caching ----------\n",
    "    def find_cached_run(self, cache_key: str) -> Optional[int]:\n",
    "        row = self.conn.execute(\"SELECT id FROM runs WHERE cache_key=? AND status='done'\", (cache_key,)).fetchone()\n",
    "        return int(row[0]) if row else None\n",
    "\n",
    "    def create_run(self, pipeline_name: str, params: Dict[str, Any], data_fingerprint: str, cache_key: str) -> int:\n",
    "        cur = self.conn.execute(\n",
    "            \"INSERT INTO runs(pipeline_name,timestamp,params_json,data_fingerprint,cache_key,status) VALUES (?,?,?,?,?,?)\",\n",
    "            (pipeline_name, self._now(), self._json(params), data_fingerprint, cache_key, \"done\")\n",
    "        )\n",
    "        self.conn.commit()\n",
    "        return int(cur.lastrowid)\n",
    "\n",
    "    def mark_run_failed(self, pipeline_name: str, params: Dict[str, Any], data_fingerprint: str, cache_key: str) -> int:\n",
    "        cur = self.conn.execute(\n",
    "            \"INSERT OR REPLACE INTO runs(pipeline_name,timestamp,params_json,data_fingerprint,cache_key,status) VALUES (?,?,?,?,?,?)\",\n",
    "            (pipeline_name, self._now(), self._json(params), data_fingerprint, cache_key, \"failed\")\n",
    "        )\n",
    "        self.conn.commit()\n",
    "        return int(cur.lastrowid)\n",
    "\n",
    "    def log_artifact(self, run_id: int, name: str, path: str, typ: str) -> None:\n",
    "        self.conn.execute(\n",
    "            \"INSERT INTO artifacts(run_id,name,path,type) VALUES (?,?,?,?)\",\n",
    "            (run_id, name, path, typ)\n",
    "        )\n",
    "        self.conn.commit()\n",
    "\n",
    "    def log_metric(self, run_id: int, key: str, value: float) -> None:\n",
    "        self.conn.execute(\n",
    "            \"INSERT INTO metrics(run_id,key,value) VALUES (?,?,?)\",\n",
    "            (run_id, key, float(value))\n",
    "        )\n",
    "        self.conn.commit()\n",
    "\n",
    "    def best_runs(self, metric: str = \"macro_f1\", limit: int = 5) -> List[Tuple[int, float]]:\n",
    "        rows = self.conn.execute(\n",
    "            \"\"\"\n",
    "            SELECT r.id, m.value\n",
    "            FROM runs r JOIN metrics m ON m.run_id=r.id\n",
    "            WHERE r.status='done' AND m.key=?\n",
    "            ORDER BY m.value DESC\n",
    "            LIMIT ?\n",
    "            \"\"\",\n",
    "            (metric, limit)\n",
    "        ).fetchall()\n",
    "        return [(int(rid), float(val)) for rid, val in rows]\n",
    "\n",
    "STORE = Store(\"metadata.db\")  # single DB file\n",
    "# end Database                                                                                          ###\n",
    "\n",
    "\n",
    "# Model builders                                                                                        ###\n",
    "def build_keras_dense(feature_keys: List[str], n_classes: int, lr: float = 1e-3) -> tf.keras.Model:\n",
    "    inputs = [keras.layers.Input(shape=(1,), name=f) for f in feature_keys]\n",
    "    d = keras.layers.concatenate(inputs)\n",
    "    for _ in range(2):\n",
    "        d = keras.layers.Dense(8, activation=\"relu\")(d)\n",
    "    # outputs = keras.layers.Dense(n_classes, activation=\"softmax\")(d)\n",
    "    outputs = keras.layers.Dense(n_classes)(d)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr),\n",
    "        # loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_keras_densenet(\n",
    "    feature_keys: List[str],\n",
    "    n_classes: int,\n",
    "    lr: float = 1e-3,\n",
    "    num_layers: int = 8,\n",
    "    growth: int = 32,\n",
    "    dropout: float = 0.0,\n",
    "    use_bn: bool = True,\n",
    ") -> tf.keras.Model:\n",
    "    # Inputs: one scalar per feature key (same as your original)\n",
    "    inputs = [keras.layers.Input(shape=(1,), name=f) for f in feature_keys]\n",
    "\n",
    "    # Base \"stem\" tensor\n",
    "    x0 = keras.layers.Concatenate(name=\"concat_inputs\")(inputs)\n",
    "\n",
    "    # Keep a running list of features to concatenate (DenseNet-style)\n",
    "    features = [x0]\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        x = keras.layers.Concatenate(name=f\"dense_concat_{i}\")(features)\n",
    "\n",
    "        if use_bn:\n",
    "            x = keras.layers.BatchNormalization(name=f\"bn_{i}\")(x)\n",
    "\n",
    "        x = keras.layers.Dense(growth, activation=\"relu\", name=f\"dense_{i}\")(x)\n",
    "\n",
    "        if dropout and dropout > 0:\n",
    "            x = keras.layers.Dropout(dropout, name=f\"dropout_{i}\")(x)\n",
    "\n",
    "        # Add this layer's output to the feature list for future layers\n",
    "        features.append(x)\n",
    "\n",
    "    # Final classifier sees all accumulated features\n",
    "    x = keras.layers.Concatenate(name=\"final_concat\")(features)\n",
    "    # outputs = keras.layers.Dense(n_classes, name=\"logits\", activation=\"softmax\")(x)\n",
    "    outputs = keras.layers.Dense(n_classes, name=\"logits\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"DenseNetMLP\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        # loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# model support function\n",
    "def to_keras_dict(X: pd.DataFrame, feature_keys: List[str]) -> Dict[str, np.ndarray]:\n",
    "    return {c: X[c].to_numpy(dtype=np.float32).reshape(-1, 1) for c in feature_keys}\n",
    "# end Model Builders                                                                                    ###\n",
    "\n",
    "# Plotting: loss | accuracy | confusion                                                                 ###\n",
    "def plot_training_summary(history: keras.callbacks.History, cm: np.ndarray, labels: List[str], outpath: Path) -> None:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "    # Loss\n",
    "    axes[0].plot(history.history.get(\"loss\", []), label=\"train\")\n",
    "    if \"val_loss\" in history.history:\n",
    "        axes[0].plot(history.history[\"val_loss\"], label=\"val\")\n",
    "    axes[0].set_title(\"Loss\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Accuracy\n",
    "    # Keras metric key can be \"acc\" or \"sparse_categorical_accuracy\"/\"accuracy\" depending on setup\n",
    "    acc_key = \"acc\" if \"acc\" in history.history else (\"accuracy\" if \"accuracy\" in history.history else None)\n",
    "    val_acc_key = \"val_acc\" if \"val_acc\" in history.history else (\"val_accuracy\" if \"val_accuracy\" in history.history else None)\n",
    "    if acc_key:\n",
    "        axes[1].plot(history.history.get(acc_key, []), label=\"train\")\n",
    "    if val_acc_key:\n",
    "        axes[1].plot(history.history.get(val_acc_key, []), label=\"val\")\n",
    "    axes[1].set_title(\"Accuracy\")\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Confusion matrix heatmap\n",
    "    im = axes[2].imshow(cm)\n",
    "    axes[2].set_title(\"Confusion Matrix\")\n",
    "    axes[2].set_xticks(range(len(labels)))\n",
    "    axes[2].set_yticks(range(len(labels)))\n",
    "    axes[2].set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    axes[2].set_yticklabels(labels)\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            axes[2].text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\")\n",
    "\n",
    "    fig.colorbar(im, ax=axes[2], fraction=0.046)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outpath, dpi=140)\n",
    "    plt.close(fig)\n",
    "# end Plotting: loss | accuracy | confusion                                                             ###\n",
    "\n",
    "\n",
    "# Core pipeline execution                                                                               ###\n",
    "ART_ROOT = Path(\"artifacts\")\n",
    "ART_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "def _data_fingerprint(df: pd.DataFrame) -> str:\n",
    "    # Stable content hash for caching (OK for small/medium data).\n",
    "    # For huge data, hash the file bytes or an external dataset version id instead.\n",
    "    h = pd.util.hash_pandas_object(df, index=True).values.tobytes()\n",
    "    return hashlib.sha256(h).hexdigest()\n",
    "\n",
    "def _preprocess(df: pd.DataFrame, label: str, numeric: List[str], categorical: List[str]) -> Tuple[pd.DataFrame, np.ndarray, List[str], Dict[str, Dict[str, int]]]:\n",
    "    df = df.copy().replace(r\"^\\s*$\", np.nan, regex=True)\n",
    "\n",
    "    # Label -> id\n",
    "    label_vocab = sorted(df[label].dropna().unique().tolist())\n",
    "    label_to_id = {v: i for i, v in enumerate(label_vocab)}\n",
    "    y = df[label].map(label_to_id).astype(\"int32\").to_numpy()\n",
    "\n",
    "    # Numeric: coerce + median impute\n",
    "    for c in numeric:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        df[c] = df[c].fillna(df[c].median()).astype(\"float32\")\n",
    "\n",
    "    # Categorical: vocab -> ids, reserve 0 for UNK\n",
    "    cat_maps: Dict[str, Dict[str, int]] = {}\n",
    "    for c in categorical:\n",
    "        vocab = sorted(df[c].dropna().astype(str).unique().tolist())\n",
    "        m = {v: (i + 1) for i, v in enumerate(vocab)}  # start at 1\n",
    "        cat_maps[c] = m\n",
    "        df[c] = df[c].astype(\"string\").fillna(\"UNK\").map(lambda x: m.get(str(x), 0)).astype(\"float32\")\n",
    "\n",
    "    features = numeric + categorical\n",
    "    X = df[features]\n",
    "    return X, y, label_vocab, cat_maps\n",
    "\n",
    "def execute_pipeline(pipeline_name: str) -> Dict[str, Any]:\n",
    "    spec = STORE.get_pipeline_spec(pipeline_name)\n",
    "    fs = STORE.get_feature_set(spec[\"feature_set\"])\n",
    "\n",
    "    label = fs[\"label\"]\n",
    "    numeric = fs[\"numeric\"]\n",
    "    categorical = fs[\"categorical\"]\n",
    "    features = numeric + categorical\n",
    "\n",
    "    # Load\n",
    "    df = pd.read_csv(spec[\"csv\"])\n",
    "    df = df[[label] + features]\n",
    "\n",
    "    # Fingerprint + cache key\n",
    "    data_fp = _data_fingerprint(df)\n",
    "\n",
    "    hyper = spec[\"hyperparams\"]\n",
    "    # Defaults if omitted\n",
    "    seed = int(hyper.get(\"seed\", 0))\n",
    "    epochs = int(hyper.get(\"epochs\", 30))\n",
    "    batch_size = int(hyper.get(\"batch_size\", 32))\n",
    "    lr = float(hyper.get(\"lr\", 1e-3))\n",
    "    test_size = float(hyper.get(\"test_size\", 0.2))\n",
    "\n",
    "    params = {\n",
    "        \"pipeline_name\": pipeline_name,\n",
    "        \"csv\": spec[\"csv\"],\n",
    "        \"feature_set\": spec[\"feature_set\"],\n",
    "        \"model_type\": spec[\"model_type\"],\n",
    "        \"hyperparams\": {\"seed\": seed, \"epochs\": epochs, \"batch_size\": batch_size, \"lr\": lr, \"test_size\": test_size},\n",
    "        \"features\": {\"label\": label, \"numeric\": numeric, \"categorical\": categorical},\n",
    "        \"data_fingerprint\": data_fp,\n",
    "    }\n",
    "    cache_key = hashlib.sha256(Store._json(params).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    cached_run_id = STORE.find_cached_run(cache_key)\n",
    "    if cached_run_id is not None:\n",
    "        run_dir = ART_ROOT / f\"run_{cached_run_id}\"\n",
    "        return {\n",
    "            \"status\": \"cached\",\n",
    "            \"run_id\": cached_run_id,\n",
    "            \"run_dir\": str(run_dir),\n",
    "            \"message\": f\"Using cached run_{cached_run_id} (cache_key match).\"\n",
    "        }\n",
    "\n",
    "    # Preprocess + split\n",
    "    X, y, label_vocab, cat_maps = _preprocess(df, label, numeric, categorical)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    try:\n",
    "        if spec[\"model_type\"] == \"keras_dense_2x8\":\n",
    "            model = build_keras_dense(features, n_classes=len(label_vocab), lr=lr)\n",
    "        elif spec[\"model_type\"] == \"keras_densenet\":\n",
    "            model = build_keras_densenet(features, n_classes=len(label_vocab), lr=lr)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model_type: {spec['model_type']}\")\n",
    "\n",
    "        history = model.fit(\n",
    "            to_keras_dict(X_train, features), y_train,\n",
    "            validation_data=(to_keras_dict(X_test, features), y_test),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        # Eval\n",
    "        logits = model.predict(to_keras_dict(X_test, features), verbose=0)\n",
    "        y_pred = logits.argmax(axis=1).astype(\"int32\")\n",
    "\n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        macro_f1 = float(report[\"macro avg\"][\"f1-score\"])\n",
    "        acc = float(report[\"accuracy\"])\n",
    "\n",
    "        # Persist run + artifacts\n",
    "        run_id = STORE.create_run(pipeline_name, params=params, data_fingerprint=data_fp, cache_key=cache_key)\n",
    "        run_dir = ART_ROOT / f\"run_{run_id}\"\n",
    "        run_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Save model\n",
    "        model_path = run_dir / \"model.keras\"\n",
    "        model.save(model_path)\n",
    "\n",
    "        # Save metrics + spec snapshot\n",
    "        with open(run_dir / \"pipeline_params.json\", \"w\") as f:\n",
    "            json.dump(params, f, indent=2)\n",
    "        with open(run_dir / \"metrics.json\", \"w\") as f:\n",
    "            json.dump({\"report\": report, \"confusion_matrix\": cm.tolist(), \"macro_f1\": macro_f1, \"accuracy\": acc}, f, indent=2)\n",
    "        with open(run_dir / \"label_vocab.json\", \"w\") as f:\n",
    "            json.dump(label_vocab, f, indent=2)\n",
    "        with open(run_dir / \"categorical_maps.json\", \"w\") as f:\n",
    "            json.dump({k: v for k, v in cat_maps.items()}, f, indent=2)\n",
    "\n",
    "        # Plot training summary\n",
    "        plot_path = run_dir / \"training_summary.png\"\n",
    "        plot_training_summary(history, cm, label_vocab, plot_path)\n",
    "\n",
    "        # Log artifacts/metrics\n",
    "        STORE.log_artifact(run_id, \"model\", str(model_path), \"keras\")\n",
    "        STORE.log_artifact(run_id, \"training_summary\", str(plot_path), \"figure\")\n",
    "        STORE.log_artifact(run_id, \"metrics\", str(run_dir / \"metrics.json\"), \"json\")\n",
    "        STORE.log_metric(run_id, \"macro_f1\", macro_f1)\n",
    "        STORE.log_metric(run_id, \"accuracy\", acc)\n",
    "\n",
    "        return {\n",
    "            \"status\": \"trained\",\n",
    "            \"run_id\": run_id,\n",
    "            \"run_dir\": str(run_dir),\n",
    "            \"macro_f1\": macro_f1,\n",
    "            \"accuracy\": acc,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        failed_run_id = STORE.mark_run_failed(pipeline_name, params=params, data_fingerprint=data_fp, cache_key=cache_key)\n",
    "        raise\n",
    "\n",
    "\n",
    "def process_next_job() -> Optional[Dict[str, Any]]:\n",
    "    job = STORE.pop_next_job()\n",
    "    if not job:\n",
    "        print(\"No queued jobs.\")\n",
    "        return None\n",
    "\n",
    "    jid = job[\"id\"]\n",
    "    name = job[\"pipeline_name\"]\n",
    "    try:\n",
    "        result = execute_pipeline(name)\n",
    "        STORE.finish_job(jid, \"done\", None)\n",
    "        print(f\"Job {jid} done -> {result.get('status')} | run_{result.get('run_id')} | {result.get('run_dir')}\")\n",
    "        if result.get(\"status\") == \"trained\":\n",
    "            print(f\"  macro_f1={result['macro_f1']:.4f} accuracy={result['accuracy']:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {result.get('message','')}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        STORE.finish_job(jid, \"failed\", str(e))\n",
    "        print(f\"Job {jid} failed: {e}\")\n",
    "        return None\n",
    "# end Core pipeline execution                                                                           ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76825eb0",
   "metadata": {},
   "source": [
    "A user-defined use of the databse and model system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "398d543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xk/tyv38sr17x77hmcdnnq8vrrc0000gn/T/ipykernel_71110/4154292643.py:106: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().isoformat()\n"
     ]
    }
   ],
   "source": [
    "# Example \"penguins\" config (TFX tutorial-style)\n",
    "STORE.register_feature_set(\n",
    "    name=\"penguins_features_v1\",\n",
    "    label=\"species\",\n",
    "    numeric=[\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\",\"body_mass_g\",\"year\"],\n",
    "    categorical=[\"island\",\"sex\"],\n",
    ")\n",
    "STORE.register_feature_set(\n",
    "    name=\"penguins_features_labelled_tfx\",\n",
    "    label=\"species\",\n",
    "    numeric=[\"culmen_length_mm\", \"culmen_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"],\n",
    "    categorical=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91e3f3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xk/tyv38sr17x77hmcdnnq8vrrc0000gn/T/ipykernel_71110/4154292643.py:106: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().isoformat()\n",
      "/var/folders/xk/tyv38sr17x77hmcdnnq8vrrc0000gn/T/ipykernel_71110/4154292643.py:106: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 3 done -> trained | run_1 | artifacts/run_1\n",
      "  macro_f1=0.8530 accuracy=0.8955\n",
      "\n",
      "Top runs by macro_f1:\n",
      "  run_1: 0.8530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xk/tyv38sr17x77hmcdnnq8vrrc0000gn/T/ipykernel_71110/4154292643.py:106: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().isoformat()\n"
     ]
    }
   ],
   "source": [
    "STORE.register_pipeline_spec(\n",
    "    name=\"penguins_keras_dense_lr=1e-3\",\n",
    "    csv='https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv',\n",
    "    feature_set=\"penguins_features_labelled_tfx\",\n",
    "    model_type=\"keras_dense_2x8\",\n",
    "    hyperparams={\"seed\": 1337, \"epochs\": 30, \"batch_size\": 16, \"lr\": 1e-3, \"test_size\": 0.2},\n",
    ")\n",
    "\n",
    "# Queue + run one job (comment these out if you only want to define config now)\n",
    "job_id = STORE.enqueue_job(\"penguins_keras_dense_lr=1e-3\", resources={\"runner\": \"local\", \"device\": \"cpu\"})\n",
    "process_next_job()\n",
    "\n",
    "# Optional: quick leaderboard\n",
    "best = STORE.best_runs(metric=\"macro_f1\", limit=5)\n",
    "if best:\n",
    "    print(\"\\nTop runs by macro_f1:\")\n",
    "    for rid, score in best:\n",
    "        print(f\"  run_{rid}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e20c999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xk/tyv38sr17x77hmcdnnq8vrrc0000gn/T/ipykernel_71110/4154292643.py:106: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 4 done -> trained | run_2 | artifacts/run_2\n",
      "  macro_f1=1.0000 accuracy=1.0000\n",
      "\n",
      "Top runs by macro_f1:\n",
      "  run_2: 1.0000\n",
      "  run_1: 0.8530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xk/tyv38sr17x77hmcdnnq8vrrc0000gn/T/ipykernel_71110/4154292643.py:106: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().isoformat()\n",
      "/var/folders/xk/tyv38sr17x77hmcdnnq8vrrc0000gn/T/ipykernel_71110/4154292643.py:106: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().isoformat()\n"
     ]
    }
   ],
   "source": [
    "STORE.register_pipeline_spec(\n",
    "    name=\"penguins_keras_densenet\",\n",
    "    csv='https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv',\n",
    "    feature_set=\"penguins_features_labelled_tfx\",\n",
    "    model_type=\"keras_densenet\",\n",
    "    hyperparams={\"seed\": 1337, \"epochs\": 100, \"batch_size\": 8, \"lr\": 1e-5, \"test_size\": 0.2},\n",
    ")\n",
    "\n",
    "# Queue + run one job (comment these out if you only want to define config now)\n",
    "job_id = STORE.enqueue_job(\"penguins_keras_densenet\", resources={\"runner\": \"local\", \"device\": \"cpu\"})\n",
    "process_next_job()\n",
    "\n",
    "# quick leaderboard\n",
    "best = STORE.best_runs(metric=\"macro_f1\", limit=5)\n",
    "if best:\n",
    "    print(\"\\nTop runs by macro_f1:\")\n",
    "    for rid, score in best:\n",
    "        print(f\"  run_{rid}: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penguins_keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
